"""
Musubi Tuner Z-Image LoRA Training Config Template

Generates TOML dataset configuration files for zimage_train_network.py
"""

import os


def generate_dataset_config(
    image_folder: str,
    resolution: int = 960,
    batch_size: int = 1,
    enable_bucket: bool = True,
    bucket_no_upscale: bool = False,
    num_repeats: int = 10,
    cache_directory: str | None = None,
) -> str:
    """
    Generate a TOML dataset config file for Musubi Tuner Z-Image training.

    Args:
        num_repeats: How many times to repeat each image per epoch.
                     Higher = fewer epochs for same step count, less overhead.
    Returns the config as a TOML string.
    """

    # Escape backslashes for TOML on Windows
    image_folder_escaped = image_folder.replace('\\', '/')
    cache_dir_escaped = cache_directory.replace('\\', '/') if cache_directory else None

    cache_line = f'cache_directory = "{cache_dir_escaped}"' if cache_dir_escaped else ''

    config = f'''# Musubi Tuner Z-Image Dataset Config
# Generated by ComfyUI Musubi Z-Image LoRA Trainer

[general]
resolution = [{resolution}, {resolution}]
batch_size = {batch_size}
enable_bucket = {str(enable_bucket).lower()}
bucket_no_upscale = {str(bucket_no_upscale).lower()}
caption_extension = ".txt"

[[datasets]]
image_directory = "{image_folder_escaped}"
{cache_line}
num_repeats = {num_repeats}
'''

    return config


def save_config(config_content: str, config_path: str):
    """Save config content to a TOML file."""
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)


# VRAM mode presets for Z-Image (Musubi Tuner)
# Max/Medium have fp8 variants, Low/Min are always fp8 with block offloading
# Note: Musubi Tuner ALWAYS requires pre-caching latents and text encoder outputs
MUSUBI_ZIMAGE_VRAM_PRESETS = {
    "Max (1256px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": False,
        "fp8_llm": False,
        "blocks_to_swap": 0,
        "resolution": 1256,
    },
    "Max (1256px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 0,
        "resolution": 1256,
    },
    "Max (1256px) fp8 offload": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 1256,
    },
    "Medium (1024px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": False,
        "fp8_llm": False,
        "blocks_to_swap": 0,
        "resolution": 1024,
    },
    "Medium (1024px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 0,
        "resolution": 1024,
    },
    "Medium (1024px) fp8 offload": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 1024,
    },
    "Low (768px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 768,
    },
    "Min (512px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 28,
        "resolution": 512,
    },
}
