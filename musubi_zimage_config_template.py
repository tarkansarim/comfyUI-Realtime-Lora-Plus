"""
Musubi Tuner Z-Image LoRA Training Config Template

Generates TOML dataset configuration files for zimage_train_network.py
"""

import os


def generate_dataset_config(
    image_folder: str,
    resolution: int = 960,
    batch_size: int = 1,
    enable_bucket: bool = True,
    bucket_no_upscale: bool = False,
    num_repeats: int = 10,
    cache_directory: str | None = None,
    use_relative_paths: bool = True,
    subprocess_cwd: str | None = None,
) -> str:
    """
    Generate a TOML dataset config file for Musubi Tuner Z-Image training.

    Args:
        num_repeats: How many times to repeat each image per epoch.
                     Higher = fewer epochs for same step count, less overhead.
        use_relative_paths: If True, convert absolute paths to relative paths
                            based on subprocess_cwd. This makes configs portable
                            when folders are renamed/moved.
        subprocess_cwd: The working directory used when running the training subprocess.
                        Paths will be made relative to this directory.
    Returns the config as a TOML string.
    """
    
    # Convert to relative paths if requested (makes config portable across folder renames)
    if use_relative_paths and subprocess_cwd:
        # Get relative paths from the subprocess working directory
        try:
            image_folder_rel = os.path.relpath(image_folder, subprocess_cwd)
            cache_dir_rel = os.path.relpath(cache_directory, subprocess_cwd) if cache_directory else None
            # Use relative paths (with ./ prefix for clarity)
            image_folder = "./" + image_folder_rel.replace('\\', '/')
            cache_directory = "./" + cache_dir_rel.replace('\\', '/') if cache_dir_rel else None
        except ValueError:
            # os.path.relpath fails if paths are on different drives on Windows
            # Fall back to absolute paths
            pass

    # Escape backslashes for TOML on Windows
    image_folder_escaped = image_folder.replace('\\', '/')
    cache_dir_escaped = cache_directory.replace('\\', '/') if cache_directory else None

    cache_line = f'cache_directory = "{cache_dir_escaped}"' if cache_dir_escaped else ''

    config = f'''# Musubi Tuner Z-Image Dataset Config
# Generated by ComfyUI Musubi Z-Image LoRA Trainer

[general]
resolution = [{resolution}, {resolution}]
batch_size = {batch_size}
enable_bucket = {str(enable_bucket).lower()}
bucket_no_upscale = {str(bucket_no_upscale).lower()}
caption_extension = ".txt"

[[datasets]]
image_directory = "{image_folder_escaped}"
{cache_line}
num_repeats = {num_repeats}
'''

    return config


def generate_multi_dataset_config(
    datasets: list[dict],
    resolution: int = 960,
    batch_size: int = 1,
    enable_bucket: bool = True,
    bucket_no_upscale: bool = False,
    use_relative_paths: bool = True,
    subprocess_cwd: str | None = None,
) -> str:
    """
    Generate a TOML dataset config file with multiple datasets (for multi-concept training).

    Args:
        datasets: List of dicts, each with:
            - image_folder: path to images
            - cache_directory: path for cache files
            - num_repeats: repeat count for this concept
        resolution: Image resolution
        batch_size: Batch size per dataset
        enable_bucket: Enable bucketed resizing
        bucket_no_upscale: If True, don't upscale small images in buckets
        use_relative_paths: Convert paths to relative paths for portability
        subprocess_cwd: Working directory for relative path calculation
    
    Returns the config as a TOML string.
    """
    
    config_lines = [
        "# Musubi Tuner Z-Image Dataset Config (Multi-Concept)",
        "# Generated by ComfyUI Musubi Z-Image LoRA Trainer",
        "",
        "[general]",
        f"resolution = [{resolution}, {resolution}]",
        f"batch_size = {batch_size}",
        f"enable_bucket = {str(enable_bucket).lower()}",
        f"bucket_no_upscale = {str(bucket_no_upscale).lower()}",
        'caption_extension = ".txt"',
        "",
    ]
    
    for ds in datasets:
        image_folder = ds['image_folder']
        cache_directory = ds.get('cache_directory')
        ds_num_repeats = ds.get('num_repeats', 1)
        
        # Convert to relative paths if requested
        if use_relative_paths and subprocess_cwd:
            try:
                image_folder_rel = os.path.relpath(image_folder, subprocess_cwd)
                image_folder = "./" + image_folder_rel.replace('\\', '/')
                if cache_directory:
                    cache_dir_rel = os.path.relpath(cache_directory, subprocess_cwd)
                    cache_directory = "./" + cache_dir_rel.replace('\\', '/')
            except ValueError:
                # os.path.relpath fails if paths are on different drives on Windows
                pass
        
        # Escape backslashes for TOML on Windows
        image_folder_escaped = image_folder.replace('\\', '/')
        cache_dir_escaped = cache_directory.replace('\\', '/') if cache_directory else None
        
        config_lines.append("[[datasets]]")
        config_lines.append(f'image_directory = "{image_folder_escaped}"')
        if cache_dir_escaped:
            config_lines.append(f'cache_directory = "{cache_dir_escaped}"')
        config_lines.append(f"num_repeats = {ds_num_repeats}")
        config_lines.append("")
    
    return "\n".join(config_lines)


def save_config(config_content: str, config_path: str):
    """Save config content to a TOML file."""
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)


# VRAM mode presets for Z-Image (Musubi Tuner)
# Max/Medium have fp8 variants, Low/Min are always fp8 with block offloading
# Note: Musubi Tuner ALWAYS requires pre-caching latents and text encoder outputs
MUSUBI_ZIMAGE_VRAM_PRESETS = {
    "Max (1256px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": False,
        "fp8_llm": False,
        "blocks_to_swap": 0,
        "resolution": 1256,
    },
    "Max (1256px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": False,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 0,
        "resolution": 1256,
    },
    "Max (1256px) fp8 offload": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 1256,
    },
    "Medium (1024px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": False,
        "fp8_llm": False,
        "blocks_to_swap": 0,
        "resolution": 1024,
    },
    "Medium (1024px) fp8": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 0,
        "resolution": 1024,
    },
    "Medium (1024px) fp8 offload": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 1024,
    },
    "Low (768px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 14,
        "resolution": 768,
    },
    "Min (512px)": {
        "optimizer": "adamw8bit",
        "mixed_precision": "bf16",
        "batch_size": 1,
        "gradient_checkpointing": True,
        "fp8_scaled": True,
        "fp8_llm": True,
        "blocks_to_swap": 28,
        "resolution": 512,
    },
}
